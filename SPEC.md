### **提案：LLMデータ分析ツール仕様**

#### **1. 概要**

本ツールは、大規模なテキストファイル（プレーンテキスト、JSONLなど）を、指定されたLLMのコンテキストウィンドウサイズに応じて自動的に分割し、各チャンクを分析します。その後、すべての中間分析結果を統合し、最終的なサマリーレポートを生成するGo製のコマンドラインツールです。

---
### **変更履歴**

開発中に以下の仕様変更が行われました。

*   **APIキーの省略に対応 (2025/10/31):** 当初APIキーは必須でしたが、ローカルLLMなどAPIキーが不要なエンドポイントに対応するため、設定ファイルの`api_key_env`を`""`（空文字列）にすることで、`Authorization`ヘッダーを送信しないように仕様を変更しました。
*   **`version`コマンドの追加 (2025/10/31):** ビルド時にgitタグからバージョンを埋め込み、それを表示する`version`コマンドを追加しました。
*   **`Makefile`の導入 (2025/10/31):** 開発ビルド、クロスプラットフォームのリリースビルド、テスト、クリーンアップを自動化する`Makefile`を導入しました。
*   **要約ロジックの変更 (2025/10/31):** 当初の再帰処理では、特定の条件下で無限ループに陥る可能性があったため、最大反復回数を設けた反復処理（イテレーティブアプローチ）に設計を変更し、安定性を向上させました。
*   **設定読み込みタイミングの変更 (2025/10/31):** テスト実行時に動的に生成される設定ファイルを正しく読み込むため、設定ファイルの読み込みタイミングを`init()`から`RunE()`の実行開始時に変更しました。

---

#### **2. 機能要件**

*   **データ入力:**
    *   分析対象のファイルパスをコマンドライン引数として受け取ります。
    *   入力形式はプレーンテキストまたはJSONL形式をサポートします。

*   **LLM設定:**
    *   設定ファイル（例: `config.yaml`）または環境変数で、複数のLLMエンドポイントを定義できます。
    *   各エンドポイント設定には以下の情報を含みます。
        *   `name`: 設定の識別名 (例: `openai_gpt4`, `local_mistral`)
        *   `endpoint_url`: APIエンドポイントのURL
        *   `api_key_env`: APIキーが格納されている環境変数名 (例: `OPENAI_API_KEY`)。**APIキーが不要な場合は`""`（空文字列）を設定します。**
        *   `model`: 使用するモデル名 (例: `gpt-4o`, `llama3-70b`)
        *   `context_window_size`: モデルの最大コンテキストウィンドウ（トークン数）
        *   `chunk_size`: データ分割時の各チャンクの最大トークン数。`context_window_size`より小さい必要があります。

*   **プロンプト設定:**
    *   データ分析用のプロンプト（各チャンクに適用）をファイルから読み込みます。
    *   最終レポート生成用のプロンプト（中間結果の要約に適用）をファイルから読み込みます。

*   **処理フロー:**
    1.  **初期化:** コマンドライン引数を解析します。
    2.  **設定読み込み:** `--config`フラグで指定された設定ファイルを読み込みます。
    3.  **データ読み込みと分割:**
        *   入力ファイルを読み込みます。
        *   Go言語用の`tiktoken`ライブラリを利用してテキストをトークンに変換し、指定された`chunk_size`に基づいてデータを分割します。
        *   JSONLの場合は、複数行をまとめて1チャンクとしますが、1行が`chunk_size`を超える場合はエラーとします。
    4.  **並列分析 (Map処理):**
        *   分割された各データチャンクに対して、Goroutineを用いて並列でLLM APIを呼び出し、分析を実行します。
        *   各API呼び出しでは、ユーザー指定の「データ分析用プロンプト」とデータチャンクをLLMに送信します。
        *   LLMからの分析結果（テキスト）を一時ディレクトリに個別のファイルとして保存します（例: `chunk_1.txt`, `chunk_2.txt`, ...）。
    5.  **結果の集約 (Reduce処理):**
        *   すべての中間分析結果ファイルを読み込み、内容を結合します。
        *   結合した中間結果とユーザー指定の「最終レポート生成用プロンプト」を使い、再度LLM APIを呼び出します。
        *   **[変更]** **反復的要約:** 結合した中間結果のトークン数がコンテキストウィンドウを超える場合は、中間結果自体をさらに分割・要約する処理を、結果が十分に小さくなるまで反復します。無限ループを防ぐため、最大反復回数（10回）を設定しています。
    6.  **出力:**
        *   最終的な分析レポートを標準出力に表示するか、指定されたファイルに出力します。

*   **一時ファイル管理:**
    *   `--temp-dir`フラグで一時ディレクトリのパスを指定できます。
    *   指定がない場合、OSの一時ディレクトリ内にランダムな名前のディレクトリを自動で作成します。
    *   `--keep-temp-dir`フラグが指定されていない限り、処理完了後に自動作成した一時ディレクトリはクリーンアップ（削除）します。

#### **3. コマンドラインインターフェース（CLI）設計案**

```bash
go-data-analyzer [flags] <input_file_path>
go-data-analyzer [command]
```

**コマンド:**
*   `version`: バージョン番号を表示します。

**フラグ:**

*   `--config, -c` (string): 設定ファイルのパス (デフォルト: `config.yaml`)
*   `--endpoint-name, -e` (string): 使用するLLMエンドポイントの名前（設定ファイルで定義）
*   `--analysis-prompt-file` (string): データ分析用プロンプトが書かれたファイルのパス **(必須)**
*   `--summary-prompt-file` (string): 最終レポート生成用プロンプトが書かれたファイルのパス **(必須)**
*   `--output, -o` (string): 出力ファイルのパス（指定がなければ標準出力）。
*   `--temp-dir` (string): 中間ファイルを保存する一時ディレクトリのパス。
*   `--keep-temp-dir` (bool): 処理終了後も一時ディレクトリを保持するかどうか。
*   `--verbose, -v` (bool): 詳細なログ（どのチャンクを処理しているかなど）を出力する。

#### **4. ビルドとテスト**

**[追加]** `Makefile`を導入し、以下のコマンドで各種操作を実行できます。

*   `make build`: 開発用バイナリを`./bin`に生成します。
*   `make test`: ユニットテストを実行します。
*   `make release`: 各種OS・アーキテクチャ向けのリリースパッケージを`./release`に生成します。
*   `make clean`: ビルド成果物を削除します。

---

### **開発計画**

以下のマイルストーンに沿って開発を進めました。

1.  **フェーズ1: プロジェクト基盤の構築 (完了)**
    *   [x] Goプロジェクトの初期化 (`go mod init`)。
    *   [x] CLIフレームワーク `cobra` を導入し、上記フラグの定義と基本的な引数処理を実装。
    *   [x] `Viper` などのライブラリを使い、`config.yaml` の読み込み処理を実装。
    *   [x] OpenAI互換APIを呼び出すための基本的なHTTPクライアントを実装。

2.  **フェーズ2: データ分割と単一分析機能 (完了)**
    *   [x] `tiktoken` ライブラリを導入し、テキストを指定トークン数で分割するロジックを実装。
    *   [x] JSONL形式を考慮した分割ロジックを実装。
    *   [x] 1つのデータチャンクとプロンプトを使い、LLMで分析を実行して結果を返すコア機能を実装。

3.  **フェーズ3: 並列処理と中間ファイル管理 (完了)**
    *   [x] Goroutineと`sync.WaitGroup`を利用して、複数のデータチャンクを並列で分析する処理を実装。
    *   [x] 分析結果を一時ファイルに保存・管理する機能を実装。
    *   [x] `--temp-dir`, `--keep-temp-dir` フラグに応じた一時ディレクトリの作成・削除処理を実装。

4.  **フェーズ4: 結果の集約と最終レポート生成 (完了)**
    *   [x] すべての中間ファイルを読み込み、結合する処理を実装。
    *   [x] 結合した中間結果とサマリープロンプトを使い、最終レポートを生成する機能（Reduce処理）を実装。
    *   [x] 中間結果がコンテキストウィンドウを超える場合の反復的要約ロジックを実装。

5.  **フェーズ5: テスト、ドキュメント、リファクタリング (完了)**
    *   [x] 主要機能に対するユニットテストと、ツール全体を通した結合テストを作成。
    *   [x] `README.md` に、ツールの目的、インストール方法、設定方法、CLIの使用例などを詳細に記述。
    *   [x] エラーハンドリングを強化し、全体的なコードの可読性と保守性を向上。